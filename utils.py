# -*- coding: utf-8 -*-
"""utils.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E-q3NM-fQYGhf2qOTj-Ir1ekZ9A1jCqr
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.feature_selection import chi2
from sklearn.decomposition import PCA
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report ,ConfusionMatrixDisplay
from sklearn.model_selection import cross_val_score, KFold ,GridSearchCV
from six import StringIO
from sklearn.tree import export_graphviz
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB ,MultinomialNB,BernoulliNB
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from sklearn.svm import SVC
import pydotplus
from IPython.display import Image
import warnings
warnings.filterwarnings('ignore')

def scale_and_concat(X, y):

    mm = MinMaxScaler()

    X_mm = mm.fit_transform(X)
    mm_df = pd.DataFrame(X_mm, columns=X.columns)

    y = y.reset_index().drop("index", axis = 1)

    df = pd.concat([mm_df, y], axis=1)

    X1 = df.iloc[:, :-1]
    y1 = df.iloc[:, [-1]]

    return X1 ,y1

def feature_selection_chi2(X, y, k=11):

    X1,y1 = scale_and_concat(X, y)

    # Apply SelectKBest class to select top n features
    bestfeatures = SelectKBest(score_func=chi2, k=k)
    fit = bestfeatures.fit(X1, y1)
    dfscores = pd.DataFrame(fit.scores_)
    dfcolumns = pd.DataFrame(X1.columns)

    featurescores = pd.concat([dfcolumns, dfscores], axis=1)
    featurescores.columns = ["feature_name", "feature_score"]
    return featurescores.nlargest(k, "feature_score")

def feature_selection_f_classif(X, y, k=11):

    X1,y1 = scale_and_concat(X, y)

    selector = SelectKBest(score_func=f_classif, k=k)  # Choose the number of top features (k)
    X_new = selector.fit_transform(X1, y1)
    selected_features_indices = selector.get_support(indices=True)

    feature_scores_df = pd.DataFrame({
        'Feature': X.columns[selected_features_indices],
        'Score': selector.scores_[selected_features_indices]
    })

    feature_scores_df = feature_scores_df.sort_values(by='Score', ascending=False)

    return feature_scores_df


def feature_selection_mutual_info_classif(X, y, k=11):

    X1,y1 = scale_and_concat(X, y)

    selector = SelectKBest(score_func=mutual_info_classif, k=11)
    X_new = selector.fit_transform(X1, y1)
    selected_features_indices = selector.get_support(indices=True)


    feature_scores_df = pd.DataFrame({
        'Feature': X.columns[selected_features_indices],
        'Score': selector.scores_[selected_features_indices]
        })

    feature_scores_df = feature_scores_df.sort_values(by='Score', ascending=False)

    return feature_scores_df

def get_most_important_features_pca(df, n_components=11):

    P = df.iloc[:, :-1].values

    model = PCA(n_components=n_components).fit(P)
    X_pca = model.transform(P)

    n_pca = model.components_.shape[0]

    most_important = [np.abs(model.components_[i]).argmax() for i in range(n_pca)]

    feature_names_t = df.columns
    most_important_names = [feature_names_t[most_important[i]] for i in range(n_pca)]

    return pd.DataFrame(most_important_names, columns=['Most_Important_Features'])


def plot_correlation_heatmap(df):
    corr = df.corr()

    fig = plt.figure(figsize=(10, 10))
    axes = fig.add_axes([0, 0, 1, 1])
    sns.heatmap(corr, annot=True)

    plt.show()

# function for tuning model parameters
def perform_grid_search(estimator, param_grid, cv, X_train, Y_train, X_test, Y_test):
    grid_search = GridSearchCV(estimator,
                           param_grid,
                           cv,
                           scoring='callable',
                           return_train_scorebool = True)

    gs = grid_search.fit(X_train, Y_train)
    gs_pred = grid_search.predict(X_test)

    results = {
        'best_params': gs.best_params_,
        'best_score': gs.best_score_,
        'classification_report': classification_report(Y_test, gs_pred),
        'accuracy_score': accuracy_score(Y_test, gs_pred),
        'best_estimator': gs.best_estimator_,
        'cv_results': gs.cv_results_,
        'grid_search_object': gs
    }

    return results

"""## Feature Scaling"""

def standardscale_features(X_train, X_val, scaler_cols, binary_features):

    sX1_train = X_train[scaler_cols]
    sX1_val = X_val[scaler_cols]


    sX2_train = X_train[binary_features].reset_index(drop=True)
    sX2_val = X_val[binary_features].reset_index(drop=True)


    scaler = StandardScaler()


    scX_train = scaler.fit_transform(sX1_train)
    sX3_train = pd.DataFrame(scX_train, columns=scaler_cols)


    scX_val = scaler.transform(sX1_val)
    sX3_val = pd.DataFrame(scX_val, columns=scaler_cols)


    sX_train = pd.concat([sX3_train, sX2_train], axis=1)
    sX_val = pd.concat([sX3_val, sX2_val], axis=1)

    return sX_train ,sX_val

# tuning decision_tree
def evaluate_decision_tree(X_train, y_train, X_val, y_val):
    acc_gini = []
    acc_entropy = []

    dtree_gini = DecisionTreeClassifier(criterion="gini")
    dtree_entropy = DecisionTreeClassifier(criterion="entropy")

    for i in range(1, 21):
        # Decision Tree with Gini criterion
        dtree_gini.set_params(max_depth=i)
        dtree_gini.fit(X_train, y_train)
        pred_gini = dtree_gini.predict(X_val)
        acc_gini.append(accuracy_score(y_val, pred_gini))

        # Decision Tree with Entropy criterion
        dtree_entropy.set_params(max_depth=i)
        dtree_entropy.fit(X_train, y_train)
        pred_entropy = dtree_entropy.predict(X_val)
        acc_entropy.append(accuracy_score(y_val, pred_entropy))

    df = pd.DataFrame({"max_depth": range(1, 21),
                       "acc_gini": acc_gini,
                       "acc_entropy": acc_entropy})

    plt.plot("max_depth", "acc_gini", data=df, label="gini")
    plt.plot("max_depth", "acc_entropy", data=df, label="entropy")
    plt.xlabel("max depth")
    plt.ylabel("accuracy")
    plt.xticks(range(1, 21))
    plt.legend()
    plt.grid()
    plt.show()

# develop decision_tree
def train_evaluate_decision_tree(X, y, X_train, y_train, X_val, y_val, criterion='gini', max_depth=None):

    classifier = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth)
    classifier.fit(X_train, y_train)
    y_pred = classifier.predict(X_val)
    y_tr = classifier.predict(X_train)
    accuracy_tr = accuracy_score(y_train, y_tr)

    accuracy = accuracy_score(y_val, y_pred)


    confusion_mat = confusion_matrix(y_val, y_pred)
    precision = precision_score(y_val, y_pred)
    recall = recall_score(y_val, y_pred)
    f1 = f1_score(y_val, y_pred)
    kfold = KFold(n_splits=10, shuffle=True, random_state=42)
    cross_val = cross_val_score(classifier, X, y, cv=kfold, scoring='accuracy')

    target_names = ['without risk', 'with risk']


    results = {
      'Model accuracy score:': accuracy,
      'train accuracy score:': accuracy_tr,
      'Confusion Matrix:': confusion_mat,
      'Precision:': precision,
      'Recall': recall,
      'best_estimator': f1,
      'cross-validation': cross_val.mean()

    }
    print(classification_report(y_val, y_pred, target_names=target_names))

      # Plot the Confusion Matrix Display
    disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat, display_labels=classifier.classes_)
    disp.plot()
    plt.show()

    return classifier , results

# visualize decision_tree
def visualize_decision_tree(classifier, feature_names=None, class_names=None):

    dot_data = StringIO()
    export_graphviz(classifier, out_file=dot_data, filled=True, precision=2,
                    feature_names=feature_names, class_names=class_names)

    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())

    return Image(graph.create_png())

# develop randomforest
def evaluate_randomforest(X, y, X_train, y_train, X_val, y_val, n_estimators,criterion, max_depth):
    classifier = RandomForestClassifier(n_estimators=n_estimators , criterion =  criterion, max_depth = max_depth)
    classifier.fit(X_train, y_train)

    y_pred = classifier.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)

    y_tr = classifier.predict(X_train)
    accuracy_tr = accuracy_score(y_train, y_tr)

    confusion_mat = confusion_matrix(y_val, y_pred)
    precision = precision_score(y_val, y_pred)
    recall = recall_score(y_val, y_pred)
    f1 = f1_score(y_val, y_pred)

    kfold = KFold(n_splits=10, shuffle=True, random_state=42)
    cross_val = cross_val_score(classifier, X, y, cv=kfold, scoring='accuracy')


    target_names = ['without risk', 'with risk']


    results = {
      'Model accuracy score:': accuracy,
      'train accuracy score:': accuracy_tr,
      'Confusion Matrix:': confusion_mat,
      'Precision:': precision,
      'Recall': recall,
      'best_estimator': f1,
      'cross-validation': cross_val.mean()

    }
    print(classification_report(y_val, y_pred, target_names=target_names))

    disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat, display_labels=classifier.classes_)
    disp.plot()
    plt.show()

    return classifier , results

#develop svm
def train_evaluate_svm(X, y, X_train, y_train, X_val, y_val,scaler_cols,binary_features, C=10, gamma=0.2, kernel='rbf'):

    X_train ,X_val =  standardscale_features(X_train, X_val , scaler_cols ,binary_features )

    svc = SVC(C=C, gamma=gamma, kernel=kernel)
    svc.fit(X_train, y_train)
    y_pred = svc.predict(X_val)

    accuracy = accuracy_score(y_val, y_pred)

    y_tr = svc.predict(X_train)
    accuracy_tr = accuracy_score(y_train, y_tr)

    confusion_mat = confusion_matrix(y_val, y_pred)
    precision = precision_score(y_val, y_pred)
    recall = recall_score(y_val, y_pred)
    f1 = f1_score(y_val, y_pred)


    kfold = KFold(n_splits=10, shuffle=True, random_state=42)
    cross_val = cross_val_score(svc, X, y, cv=kfold, scoring='accuracy')

    target_names = ['without risk', 'with risk']


    results = {
      'Model accuracy score:': accuracy,
      'train accuracy score:': accuracy_tr,
      'Confusion Matrix:': confusion_mat,
      'Precision:': precision,
      'Recall': recall,
      'best_estimator': f1,
      'cross-validation': cross_val.mean()

    }
    print(classification_report(y_val, y_pred, target_names=target_names))

    return results

##develop knn
def train_predict_evaluate_knn(X, y, X_train, y_train, X_val, y_val,scaler_cols, binary_features,n_neighbors=7):

    X_train ,X_val =  standardscale_features(X_train, X_val , scaler_cols ,binary_features )


    classifier = KNeighborsClassifier(n_neighbors=n_neighbors, metric='minkowski', p=2)
    classifier.fit(X_train, y_train)
    y_pred = classifier.predict(X_val)

    y_tr = classifier.predict(X_train)
    accuracy_tr = accuracy_score(y_train, y_tr)

    accuracy = accuracy_score(y_val, y_pred)
    confusion_mat = confusion_matrix(y_val, y_pred)
    accuracy = accuracy_score(y_val, y_pred)
    precision = precision_score(y_val, y_pred)
    recall = recall_score(y_val, y_pred)
    f1 = f1_score(y_val, y_pred)

    kfold = KFold(n_splits=10, shuffle=True, random_state=42)
    cross_val = cross_val_score(classifier, X, y, cv=kfold, scoring='accuracy')

    target_names = ['without risk', 'with risk']

    results = {
      'Model accuracy score:': accuracy,
      'train accuracy score:': accuracy_tr,
      'Confusion Matrix:': confusion_mat,
      'Precision:': precision,
      'Recall': recall,
      'best_estimator': f1,
      'cross-validation': cross_val.mean()

    }
    print(classification_report(y_val, y_pred, target_names=target_names))

    return results

#tuning and develop naive_bayes_Gaussian

def train_evaluate_naive_bayes_Gaussian(X ,y ,X_train, Y_train, X_val, y_val):

    classifier = GaussianNB()

    param_grid = {
        'var_smoothing': np.logspace(0, -9, num=100)
    }


    grid_search = GridSearchCV(estimator=classifier,
                               param_grid=param_grid,
                               cv=5,
                               scoring='accuracy',
                               verbose=1)

    grid_search.fit(X_train, Y_train)


    best_params = grid_search.best_params_


    best_classifier = GaussianNB(var_smoothing=best_params['var_smoothing'])

    best_classifier.fit(X_train, Y_train)


    y_pred = best_classifier.predict(X_val)

    accuracy = accuracy_score(y_val, y_pred)
    f1 = f1_score(y_val, y_pred)
    precision = precision_score(y_val, y_pred)
    recall = recall_score(y_val, y_pred)

    print("Test Set Metrics:")
    print("Accuracy:", accuracy)
    print("F1 Score:", f1)
    print("Precision:", precision)
    print("Recall:", recall)


    cross_val_scores = cross_val_score(best_classifier, X, y, cv=5, scoring='accuracy')

    print("\nCross-Validation Scores:", cross_val_scores)

    y_val_pred = best_classifier.predict(X_val)
    target_names = ['without risk', 'with risk']
    print("\nClassification Report on Validation Set:")
    print(classification_report(y_val, y_val_pred, target_names=target_names))

#tuning and develop naive_bayes_Multinomial

def train_evaluate_naive_bayes_Multinomial(X , y, X_train, Y_train, X_val, y_val):
    X1,y1 = scale_and_concat(X_train, Y_train)
    X2,y2 = scale_and_concat(X_val, y_val)


    classifier = MultinomialNB()


    param_grid = {
        'alpha': [0.1, 0.5, 1.0, 2.0],
        'fit_prior': [True, False],
        'class_prior': [None]
    }


    grid_search = GridSearchCV(estimator=classifier,
                               param_grid=param_grid,
                               cv=5,
                               scoring='accuracy',
                               verbose=1)

    grid_search.fit(X1,y1)


    best_params = grid_search.best_params_


    best_classifier = MultinomialNB(alpha=best_params['alpha'],
                                    fit_prior=best_params['fit_prior'],
                                    class_prior=best_params['class_prior'])


    best_classifier.fit(X1,y1)


    y_val_pred = best_classifier.predict(X2)


    accuracy = accuracy_score(y2, y_val_pred)
    f1 = f1_score(y2, y_val_pred)
    precision = precision_score(y2, y_val_pred)
    recall = recall_score(y2, y_val_pred)

    print("Validation Set Metrics:")
    print("Accuracy:", accuracy)
    print("F1 Score:", f1)
    print("Precision:", precision)
    print("Recall:", recall)


    cross_val_scores = cross_val_score(best_classifier, X, y, cv=5, scoring='accuracy')

    print("\nCross-Validation Scores:", cross_val_scores)


    target_names = ['without risk', 'with risk']
    print("\nClassification Report on Validation Set:")
    print(classification_report(y2, y_val_pred, target_names=target_names))

#tuning and develop naive_bayes_Bernoulli
def train_evaluate_naive_bayes_Bernoulli(X, y, X_train, Y_train, X_val, y_val):
    X1, y1 = scale_and_concat(X_train, Y_train)
    X2, y2 = scale_and_concat(X_val, y_val)


    classifier = BernoulliNB()


    param_grid = {
        'alpha': [0.1, 0.5, 1.0, 2.0],
        'binarize': [0.0, 0.5, 1.0],
        'fit_prior': [True, False],
        'class_prior': [None]
    }

    grid_search = GridSearchCV(estimator=classifier,
                               param_grid=param_grid,
                               cv=5,
                               scoring='accuracy',
                               verbose=1)

    grid_search.fit(X1, y1)

    best_params = grid_search.best_params_

    best_classifier = BernoulliNB(alpha=best_params['alpha'],
                                  binarize=best_params['binarize'],
                                  fit_prior=best_params['fit_prior'],
                                  class_prior=best_params['class_prior'])

    best_classifier.fit(X1, y1)

    y_val_pred = best_classifier.predict(X2)

    accuracy = accuracy_score(y2, y_val_pred)
    f1 = f1_score(y2, y_val_pred)
    precision = precision_score(y2, y_val_pred)
    recall = recall_score(y2, y_val_pred)

    print("Validation Set Metrics:")
    print("Accuracy:", accuracy)
    print("F1 Score:", f1)
    print("Precision:", precision)
    print("Recall:", recall)

    cross_val_scores = cross_val_score(best_classifier, X, y, cv=5, scoring='accuracy')

    print("\nCross-Validation Scores:", cross_val_scores)

    target_names = ['without risk', 'with risk']
    print("\nClassification Report on Validation Set:")
    print(classification_report(y2, y_val_pred, target_names=target_names))

#tuning and develop logistic_regression
def train_evaluate_logistic_regression(X, y, X_train, y_train, X_val, y_val):

    logistic_Reg = LogisticRegression()
    pipe = Pipeline(steps=[('logistic_Reg', logistic_Reg)])
    C = np.logspace(-4, 4, 50)
    penalty = ['l1', 'l2']
    parameters = {'logistic_Reg__C': C, 'logistic_Reg__penalty': penalty}


    clf_GS = GridSearchCV(pipe, parameters)
    clf_GS.fit(X_train, y_train)


    best_model = clf_GS.best_estimator_


    best_model.fit(X_train, y_train)

    y_pred = best_model.predict(X_val)

    y_tr = best_model.predict(X_train)
    accuracy_tr = accuracy_score(y_train, y_tr)

    accuracy = accuracy_score(y_val, y_pred)
    f1 = f1_score(y_val, y_pred)
    precision = precision_score(y_val, y_pred)
    recall = recall_score(y_val, y_pred)


    target_names = ['without risk', 'with risk']



    # 10-fold cross-validation
    kfold = KFold(n_splits=10)
    modelCV = LogisticRegression(C=best_model.get_params()['logistic_Reg__C'],
                                              penalty=best_model.get_params()['logistic_Reg__penalty'],
                                              random_state=0)

    cross_val = cross_val_score(modelCV, X, y, cv=kfold, scoring='accuracy')


    results = {
      "Accuracy:": accuracy,
      'train accuracy score:': accuracy_tr,
      'F1 Score:': f1,
      'Precision:': precision,
      'Recall': recall,
      'best_estimator': f1,
      'Cross-Validation Scores': cross_val.mean()
    }

    print(classification_report(y_val, y_pred, target_names=target_names))

    return results

#tuning and develop logistic_regression_net
def train_evaluate_logistic_regression_elastic_net(X, y, X_train, y_train, X_val, y_val):

    logistic_Reg = LogisticRegression(penalty='elasticnet', solver='saga')
    pipe = Pipeline(steps=[('logistic_Reg', logistic_Reg)])


    C = np.logspace(-4, 4, 50)
    l1_ratio = np.linspace(0, 1, 20)
    parameters = {'logistic_Reg__C': C, 'logistic_Reg__l1_ratio': l1_ratio}


    clf_GS = GridSearchCV(pipe, parameters)
    clf_GS.fit(X_train, y_train)


    best_model = clf_GS.best_estimator_


    best_model.fit(X_train, y_train)


    y_pred = best_model.predict(X_val)


    accuracy = accuracy_score(y_val, y_pred)
    f1 = f1_score(y_val, y_pred)
    precision = precision_score(y_val, y_pred)
    recall = recall_score(y_val, y_pred)

    y_tr = best_model.predict(X_train)
    accuracy_tr = accuracy_score(y_train, y_tr)




    target_names = ['without risk', 'with risk']



    kfold = KFold(n_splits=10)
    modelCV = LogisticRegression(C=best_model.get_params()['logistic_Reg__C'],
                                 penalty='elasticnet',
                                 l1_ratio=best_model.get_params()['logistic_Reg__l1_ratio'],
                                 solver='saga')

    cross_val = cross_val_score(modelCV, X_train, y_train, cv=kfold, scoring='accuracy')


    results = {
      "Accuracy:": accuracy,
      'train accuracy score:': accuracy_tr,
      'F1 Score:': f1,
      'Precision:': precision,
      'Recall': recall,
      'best_estimator': f1,
      'Cross-Validation Scores': cross_val.mean()
    }

    print(classification_report(y_val, y_pred, target_names=target_names))

    return results

#develop xgboost
def train_evaluate_xgboost(X, y, X_train, y_train, X_val, y_val, xgb_params):

    xgb_clf = XGBClassifier(**xgb_params)
    xgb_clf.fit(X_train, y_train)
    y_pred = xgb_clf.predict(X_val)

    accuracy = accuracy_score(y_val, y_pred)
    confusion_mat = confusion_matrix(y_val, y_pred)
    precision = precision_score(y_val, y_pred)
    recall = recall_score(y_val, y_pred)
    f1 = f1_score(y_val, y_pred)

    kfold = KFold(n_splits=10, shuffle=True, random_state=42)
    cross_val = cross_val_score(xgb_clf, X, y, cv=kfold, scoring='accuracy')

    y_tr = xgb_clf.predict(X_train)
    accuracy_tr = accuracy_score(y_train, y_tr)

    target_names = ['without risk', 'with risk']


    results = {
      'Model accuracy score:': accuracy,
      'train accuracy score:': accuracy_tr,
      'Confusion Matrix:': confusion_mat,
      'Precision:': precision,
      'Recall': recall,
      'best_estimator': f1,
      'cross-validation': cross_val.mean()
    }

    print(classification_report(y_val, y_pred, target_names=target_names))

    # Plot the Confusion Matrix Display
    disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat, display_labels=xgb_clf.classes_)
    disp.plot()
    plt.show()

    return xgb_clf , results